{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "steganography.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPaGUmA53FCp5eXNwN+eJn2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heaven-hand/GAN/blob/main/steganography.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoPsNYoWO5IV"
      },
      "source": [
        "# **Download the dataset**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGzduNguNxKz",
        "outputId": "8b06f9e9-4751-4dba-feaf-9d1981fa5ff3"
      },
      "source": [
        "!wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_HR.zip #--output-file \"{workspace_dir}/DIV2K_valid_HR.zip\"\n",
        "!wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\n",
        "#!wget http://images.cocodataset.org/zips/test2017.zip\n",
        "#!wget http://images.cocodataset.org/zips/train2017.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train2017.zip        37%[======>             ]   6.69G  36.0MB/s    eta 8m 2s  ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuPpp0FpPXhb"
      },
      "source": [
        "# **Unzip the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1LT93plOZvk",
        "outputId": "f388abae-67da-413c-9f5f-f20cafb44566"
      },
      "source": [
        "#!unzip -q \"DIV2K_valid_HR.zip\" \n",
        "!unzip -q \"DIV2K_train_HR.zip\" \n",
        "#!unzip -q \"test2017.zip\" \n",
        "#!unzip -q \"train2017.zip\" "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace DIV2K_train_HR/0103.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CelKBQygSE-k"
      },
      "source": [
        "# **初始encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj7aWFSMCSq4"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy\n",
        "\n",
        "\n",
        "class BasicEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The BasicEncoder module takes an cover image and a data tensor and combines\n",
        "    them into a steganographic image.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def _conv2d(self, in_channels, out_channels):\n",
        "        return nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "\n",
        "    def _build_models(self):\n",
        "        self.conv1 = nn.Sequential(\n",
        "            self._conv2d(3, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size + self.data_depth, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, 3),\n",
        "        )\n",
        "        return self.conv1, self.conv2, self.conv3, self.conv4\n",
        "\n",
        "    def __init__(self, data_depth, hidden_size):\n",
        "        super().__init__()\n",
        "        self.data_depth = data_depth\n",
        "        self.hidden_size = hidden_size\n",
        "        self._models = self._build_models()\n",
        "\n",
        "    def forward(self, image, data):\n",
        "        x = self._models[0](image)\n",
        "        x_1 = self._models[1](torch.cat([x] + [data], dim=1))\n",
        "        x_2 = self._models[2](x_1)\n",
        "        x_3 = self._models[3](x_2)\n",
        "        return x_3\n",
        "\n",
        "\n",
        "class ResidualEncoder(BasicEncoder):\n",
        "\n",
        "    def forward(self, image, data):\n",
        "        return image + super().forward(self, image, data)\n",
        "\n",
        "\n",
        "class DenseEncoder(ResidualEncoder):\n",
        "\n",
        "    def _build_models(self):\n",
        "        self.conv1 = nn.Sequential(\n",
        "            self._conv2d(3, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size + self.data_depth, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size * 2 +\n",
        "                         self.data_depth, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size * 3 + self.data_depth, 3)\n",
        "        )\n",
        "\n",
        "        return self.conv1, self.conv2, self.conv3, self.conv4\n",
        "\n",
        "    def forward(self, image, data):\n",
        "        x = self._models[0](image)\n",
        "        x_list = [x]\n",
        "        x_1 = self._models[1](torch.cat(x_list+[data], dim=1))\n",
        "        x_list.append(x_1)\n",
        "        x_2 = self._models[2](torch.cat(x_list+[data], dim=1))\n",
        "        x_list.append(x_2)\n",
        "        x_3 = self._models[3](torch.cat(x_list+[data], dim=1))\n",
        "        x_list.append(x_3)\n",
        "        return image + x_3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYsdHegECim_"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class BasicDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The BasicDecoder module takes an steganographic image and attempts to decode\n",
        "    the embedded data tensor.\n",
        "\n",
        "    Input: (N, 3, H, W)\n",
        "    Output: (N, D, H, W)\n",
        "    \"\"\"\n",
        "\n",
        "    def _conv2d(self, in_channels, out_channels):\n",
        "        return nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "\n",
        "    def _build_models(self):\n",
        "        self.conv1 = nn.Sequential(\n",
        "            self._conv2d(3, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.data_depth)\n",
        "        )\n",
        "\n",
        "        return self.conv1, self.conv2, self.conv3, self.conv4\n",
        "\n",
        "    def forward(self, image):\n",
        "        x = self._models[0](image)\n",
        "        x_1 = self._models[1](x)\n",
        "        x_2 = self._models[2](x_1)\n",
        "        x_3 = self._models[3](x_2)\n",
        "        return x_3\n",
        "\n",
        "    def __init__(self, data_depth, hidden_size):\n",
        "        super().__init__()\n",
        "        self.data_depth = data_depth\n",
        "        self.hidden_size = hidden_size\n",
        "        self._models = self._build_models()\n",
        "\n",
        "\n",
        "class DenseDecoder(BasicDecoder):\n",
        "\n",
        "    def _build_models(self):\n",
        "        self.conv1 = nn.Sequential(\n",
        "            self._conv2d(3, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size * 2, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size * 3, self.data_depth)\n",
        "        )\n",
        "\n",
        "        return self.conv1, self.conv2, self.conv3, self.conv4\n",
        "\n",
        "    def forward(self, image):\n",
        "        x = self._models[0](image)\n",
        "        x_list = [x]\n",
        "        x_1 = self._models[1](torch.cat(x_list, dim=1))\n",
        "        x_list.append(x_1)\n",
        "        x_2 = self._models[2](torch.cat(x_list, dim=1))\n",
        "        x_list.append(x_2)\n",
        "        x_3 = self._models[3](torch.cat(x_list, dim=1))\n",
        "        x_list.append(x_3)\n",
        "        return x_3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY77AmsIClwC"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class BasicCritic(nn.Module):\n",
        "    \"\"\"\n",
        "    The BasicCritic module takes an image and predicts whether it is a cover\n",
        "    image or a steganographic image (N, 1).\n",
        "\n",
        "    Input: (N, 3, H, W)\n",
        "    Output: (N, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    def _conv2d(self, in_channels, out_channels):\n",
        "        return nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3\n",
        "        )\n",
        "\n",
        "    def _build_models(self):\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            self._conv2d(3, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )  \n",
        "        self.conv4 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, 1)\n",
        "        )         \n",
        "\n",
        "        return self.conv1,self.conv2,self.conv3,self.conv4\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self._models = self._build_models()\n",
        "\n",
        "    def forward(self, image):\n",
        "        x = self._models[0](image)\n",
        "        x_1 = self._models[1](x)\n",
        "        x_2 = self._models[2](x_1)\n",
        "        x_3 = self._models[3](x_2)\n",
        "        return torch.mean(x_3.view(x_3.size(0), -1), dim=1)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n7QrRIsX9bG",
        "outputId": "5d1a3b88-8d8a-429d-ccf6-cb25a748e0de"
      },
      "source": [
        "!pip install pytorch_ssim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_ssim\n",
            "  Downloading pytorch_ssim-0.1.tar.gz (1.4 kB)\n",
            "Building wheels for collected packages: pytorch-ssim\n",
            "  Building wheel for pytorch-ssim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-ssim: filename=pytorch_ssim-0.1-py3-none-any.whl size=2026 sha256=74abcc331851c63d034fd635cb7d3b7c299c0c67a1dba54c590f7c70aab4bdc0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/20/09/ebf5e58bdf2560c760074cd140b7f7b0c882e216feabf1ae30\n",
            "Successfully built pytorch-ssim\n",
            "Installing collected packages: pytorch-ssim\n",
            "Successfully installed pytorch-ssim-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vsjl_wpMbsvh",
        "outputId": "89ca8ebe-f42d-44d6-a0d4-bad36dff6b35"
      },
      "source": [
        "cd /home\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "R5FigMaLcY1m",
        "outputId": "f3ea546e-bc5e-4bd9-d62f-44d7f24583e8"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/home'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF-tfYxCcbPy",
        "outputId": "f9b1469c-c141-402b-82d8-fe03d150ed2d"
      },
      "source": [
        "cd /content\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "MtZK-l-gCodg",
        "outputId": "e102e868-f28f-4a44-f410-76810f78c0f2"
      },
      "source": [
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.functional import binary_cross_entropy_with_logits, mse_loss\n",
        "import glob\n",
        "import torchvision.transforms as transforms\n",
        "#from critic import BasicCritic\n",
        "#from decoder import BasicDecoder\n",
        "#from encoder import BasicEncoder\n",
        "from torchvision import datasets, transforms\n",
        "from IPython.display import clear_output\n",
        "import torchvision\n",
        "from torch.optim import Adam\n",
        "import pytorch_ssim\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import os\n",
        "import gc\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, fnames, transform):\n",
        "        self.transform = transform\n",
        "        self.fnames = fnames\n",
        "        self.num_samples = len(self.fnames)\n",
        "    def __getitem__(self,idx):\n",
        "        fname = self.fnames[idx]\n",
        "        img = cv2.imread(fname)\n",
        "        #img = self.BGR2RGB(img) #because \"torchvision.utils.save_image\" use RGB\n",
        "        img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    #def BGR2RGB(self,img):\n",
        "    #    return cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def plot(name, train_epoch, values, save):\n",
        "    clear_output(wait=True)\n",
        "    plt.close('all')\n",
        "    fig = plt.figure()\n",
        "    fig = plt.ion()\n",
        "    fig = plt.subplot(1, 1, 1)\n",
        "    fig = plt.title('epoch: %s -> %s: %s' % (train_epoch, name, values[-1]))\n",
        "    fig = plt.ylabel(name)\n",
        "    fig = plt.xlabel('epoch')\n",
        "    fig = plt.plot(values)\n",
        "    fig = plt.grid()\n",
        "    get_fig = plt.gcf()\n",
        "    fig = plt.draw()  # draw the plot\n",
        "    fig = plt.pause(1)  # show it for 1 second\n",
        "    if save:\n",
        "        now = datetime.datetime.now()\n",
        "        get_fig.savefig('results/plots/%s_%d_%.3f_%s.png' %\n",
        "                        (name, train_epoch, values[-1], now.strftime(\"%Y-%m-%d_%H:%M:%S\")))\n",
        "\n",
        "def get_dataset(root):\n",
        "    fnames = glob.glob(os.path.join(root, '*'))\n",
        "    # resize the image to (64, 64)\n",
        "    # linearly map [0, 1] to [-1, 1]\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToPILImage(),\n",
        "         transforms.Resize((64, 64)),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3) ] )\n",
        "    dataset = FaceDataset(fnames, transform)\n",
        "    return dataset\n",
        "\n",
        "def main():\n",
        "    data_dir = '/content'\n",
        "    epochs = 5\n",
        "    data_depth = 2\n",
        "    hidden_size = 32\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    METRIC_FIELDS = [\n",
        "        'val.encoder_mse',\n",
        "        'val.decoder_loss',\n",
        "        'val.decoder_acc',\n",
        "        'val.cover_score',\n",
        "        'val.generated_score',\n",
        "        'val.ssim',\n",
        "        'val.psnr',\n",
        "        'val.bpp',\n",
        "        'train.encoder_mse',\n",
        "        'train.decoder_loss',\n",
        "        'train.decoder_acc',\n",
        "        'train.cover_score',\n",
        "        'train.generated_score',\n",
        "    ]\n",
        "\n",
        "    mu = [.5, .5, .5]\n",
        "    sigma = [.5, .5, .5]\n",
        "\n",
        "    transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                    transforms.RandomCrop(\n",
        "                                        360, pad_if_needed=True),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mu, sigma)])\n",
        "\n",
        "    #train_set = datasets.ImageFolder(os.path.join(\n",
        "    #   data_dir, \"train/\"), transform=transform)\n",
        "    train_set= get_dataset(os.path.join(data_dir, 'train','DIV2K_train_HR'))\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_set, batch_size=4, shuffle=True)\n",
        "\n",
        "    #valid_set = datasets.ImageFolder(os.path.join(\n",
        "    #    data_dir, \"valid/\"), transform=transform)\n",
        "    valid_set= get_dataset(os.path.join(data_dir, 'valid','DIV2K_valid_HR'))\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_set, batch_size=4, shuffle=False)\n",
        "\n",
        "    encoder = BasicEncoder(data_depth, hidden_size)\n",
        "    decoder = BasicDecoder(data_depth, hidden_size)\n",
        "    critic = BasicCritic(hidden_size)\n",
        "    cr_optimizer = Adam(critic.parameters(), lr=1e-4)\n",
        "    # Why add encoder parameters too?\n",
        "    en_de_optimizer = Adam(list(decoder.parameters()) +\n",
        "                           list(encoder.parameters()), lr=1e-4)\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        metrics = {field: list() for field in METRIC_FIELDS}\n",
        "        for cover, _ in tqdm(train_loader):\n",
        "            gc.collect()\n",
        "            cover = cover.to(device)\n",
        "            N, _, H, W = cover.size()\n",
        "            # sampled from the discrete uniform distribution over 0 to 2\n",
        "            payload = torch.zeros((N, data_depth, H, W),\n",
        "                                  device=device).random_(0, 2)\n",
        "            generated = encoder.forward(cover, payload)\n",
        "            cover_score = torch.mean(critic.forward(cover))\n",
        "            generated_score = torch.mean(critic.forward(generated))\n",
        "\n",
        "            cr_optimizer.zero_grad()\n",
        "            (cover_score - generated_score).backward(retain_graph=False)\n",
        "            cr_optimizer.step()\n",
        "\n",
        "            for p in critic.parameters():\n",
        "                p.data.clamp_(-0.1, 0.1)\n",
        "            metrics['train.cover_score'].append(cover_score.item())\n",
        "            metrics['train.generated_score'].append(generated_score.item())\n",
        "\n",
        "        for cover, _ in tqdm(train_loader):\n",
        "            gc.collect()\n",
        "            cover = cover.to(device)\n",
        "            N, _, H, W = cover.size()\n",
        "            # sampled from the discrete uniform distribution over 0 to 2\n",
        "            payload = torch.zeros((N, data_depth, H, W),\n",
        "                                  device=device).random_(0, 2)\n",
        "            generated = encoder.forward(cover, payload)\n",
        "            decoded = decoder.forward(generated)\n",
        "\n",
        "            encoder_mse = mse_loss(generated, cover)\n",
        "            decoder_loss = binary_cross_entropy_with_logits(decoded, payload)\n",
        "            decoder_acc = (decoded >= 0.0).eq(\n",
        "                payload >= 0.5).sum().float() / payload.numel()\n",
        "            generated_score = torch.mean(critic.forward(generated))\n",
        "\n",
        "            en_de_optimizer.zero_grad()\n",
        "            (100.0 * encoder_mse + decoder_loss +\n",
        "             generated_score).backward()  # Why 100?\n",
        "            en_de_optimizer.step()\n",
        "\n",
        "            metrics['train.encoder_mse'].append(encoder_mse.item())\n",
        "            metrics['train.decoder_loss'].append(decoder_loss.item())\n",
        "            metrics['train.decoder_acc'].append(decoder_acc.item())\n",
        "\n",
        "        for cover, _ in tqdm(valid_loader):\n",
        "            gc.collect()\n",
        "            cover = cover.to(device)\n",
        "            N, _, H, W = cover.size()\n",
        "            # sampled from the discrete uniform distribution over 0 to 2\n",
        "            payload = torch.zeros((N, data_depth, H, W),\n",
        "                                  device=device).random_(0, 2)\n",
        "            generated = encoder.forward(cover, payload)\n",
        "            decoded = decoder.forward(generated)\n",
        "\n",
        "            encoder_mse = mse_loss(generated, cover)\n",
        "            decoder_loss = binary_cross_entropy_with_logits(decoded, payload)\n",
        "            decoder_acc = (decoded >= 0.0).eq(\n",
        "                payload >= 0.5).sum().float() / payload.numel()\n",
        "            generated_score = torch.mean(critic.forward(generated))\n",
        "            cover_score = torch.mean(critic.forward(cover))\n",
        "\n",
        "            metrics['val.encoder_mse'].append(encoder_mse.item())\n",
        "            metrics['val.decoder_loss'].append(decoder_loss.item())\n",
        "            metrics['val.decoder_acc'].append(decoder_acc.item())\n",
        "            metrics['val.cover_score'].append(cover_score.item())\n",
        "            metrics['val.generated_score'].append(generated_score.item())\n",
        "            metrics['val.ssim'].append(\n",
        "                pytorch_ssim.ssim(cover, generated).item())\n",
        "            metrics['val.psnr'].append(\n",
        "                10 * torch.log10(4 / encoder_mse).item())\n",
        "            metrics['val.bpp'].append(\n",
        "                data_depth * (2 * decoder_acc.item() - 1))\n",
        "        now = datetime.datetime.now()\n",
        "        name = \"EN_DE_%+.3f_%s.dat\" % (cover_score.item(),\n",
        "                                       now.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
        "        fname = os.path.join('.', 'results/model', name)\n",
        "        states = {\n",
        "            'state_dict_critic': critic.state_dict(),\n",
        "            'state_dict_encoder': encoder.state_dict(),\n",
        "            'state_dict_decoder': decoder.state_dict(),\n",
        "            'en_de_optimizer': en_de_optimizer.state_dict(),\n",
        "            'cr_optimizer': cr_optimizer.state_dict(),\n",
        "            'metrics': metrics,\n",
        "            'train_epoch': ep,\n",
        "            'date': now.strftime(\"%Y-%m-%d_%H:%M:%S\"),\n",
        "        }\n",
        "        torch.save(states, fname)\n",
        "        plot('encoder_mse', ep, metrics['val.encoder_mse'], True)\n",
        "        plot('decoder_loss', ep, metrics['val.decoder_loss'], True)\n",
        "        plot('decoder_acc', ep, metrics['val.decoder_acc'], True)\n",
        "        plot('cover_score', ep, metrics['val.cover_score'], True)\n",
        "        plot('generated_score', ep, metrics['val.generated_score'], True)\n",
        "        plot('ssim', ep, metrics['val.ssim'], True)\n",
        "        plot('psnr', ep, metrics['val.psnr'], True)\n",
        "        plot('bpp', ep, metrics['val.bpp'], True)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    for func in [\n",
        "            lambda:os.mkdir(os.path.join('.', 'results')),\n",
        "            lambda: os.mkdir(os.path.join('.', 'results/model')),\n",
        "            lambda: os.mkdir(os.path.join('.', 'results/plots'))]:  # create directories\n",
        "        try:\n",
        "            func()\n",
        "        except Exception as error:\n",
        "            print(error)\n",
        "            continue\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 17] File exists: './results'\n",
            "[Errno 17] File exists: './results/model'\n",
            "[Errno 17] File exists: './results/plots'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-f7b3e02ba74f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-f7b3e02ba74f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMETRIC_FIELDS\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mcover\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mcover\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcover\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    }
  ]
}